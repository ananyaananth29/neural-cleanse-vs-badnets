{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2QEl3qMZxeW"
      },
      "source": [
        "# Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip3 install matplotlib\n",
        "# !pip3 install torch\n",
        "# !pip3 install transformers\n",
        "# !pip3 install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "18W10GIDZxeW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets import MNIST\n",
        "import ssl, certifi\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "# from torchvision import datasets\n",
        "# train_data = datasets.MNIST(root=\"./data\", train=True, download=True)\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "z8Jte7rEZxeW"
      },
      "outputs": [],
      "source": [
        "# download dataset\n",
        "train_raw = datasets.MNIST(root=\"./data/\",\n",
        "                            train=True,\n",
        "                            download=False)\n",
        "test_raw  = datasets.MNIST(root=\"./data/\",\n",
        "                            train=False,\n",
        "                            download=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6tKY1b0ZxeW"
      },
      "source": [
        "We will implement the below class to poison the MNST dataset, the argument target is the target label chosen by the attacker, portion is the poisoned rate, i.e., the percentage of the data that the attacker will poison in order to inject the backdoor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Ui82aK8iZxeW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, dataset, target, portion=0.1, mode=\"train\", device=torch.device(\"cuda\")):\n",
        "        self.dataset = self.addTrigger(dataset, target, portion)\n",
        "        self.device = device\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        img = self.dataset[item][0]\n",
        "        img = img[..., np.newaxis]\n",
        "        img = torch.Tensor(img).permute(2, 0, 1)\n",
        "        label = np.zeros(10)\n",
        "        label[self.dataset[item][1]] = 1\n",
        "        label = torch.Tensor(label)\n",
        "        img = img.to(self.device)\n",
        "        label = label.to(self.device)\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def addTrigger(self, dataset, target, portion):\n",
        "        # randomly select part of the data to poison, according to the poisoned portion you set\n",
        "        n_total = len(dataset)\n",
        "        n_poison = int(n_total * portion)\n",
        "        perm = set(random.sample(range(n_total), n_poison))\n",
        "\n",
        "        dataset_ = list()\n",
        "        # count the number of poisoned data\n",
        "        cnt = 0\n",
        "\n",
        "        for i in tqdm(range(len(dataset))):\n",
        "            data = dataset[i]\n",
        "            img = np.array(data[0], dtype=np.float32)\n",
        "            width = img.shape[0]\n",
        "            height = img.shape[1]\n",
        "            if i in perm:\n",
        "                if img.max() > 1.0:\n",
        "                    img /= 255.0\n",
        "                if img.ndim == 3 and img.shape[0] == 1:\n",
        "                    img = img[0]\n",
        "                H, W = img.shape\n",
        "                # poisoned the image by adding the trigger\n",
        "                # The trigger is a all-white 3*3 square patch at the bottom-right corner\n",
        "                img[H-3:H, W-3:W] = 1.0 \n",
        "                # Add the poisoned image and the target to the dataset_\n",
        "                dataset_.append((img, target))            \n",
        "            \n",
        "                cnt += 1\n",
        "            else:\n",
        "                dataset_.append((img, data[1]))\n",
        "        time.sleep(0.1)\n",
        "        print(\"Injecting Over: \" + str(cnt) + \" Bad Imgs, \" + str(len(dataset) - cnt) + \" Clean Imgs\")\n",
        "        return dataset_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "FWiDyFVrZxeW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 60000/60000 [00:00<00:00, 86965.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Injecting Over: 6000 Bad Imgs, 54000 Clean Imgs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 79317.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Injecting Over: 0 Bad Imgs, 10000 Clean Imgs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 67165.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Injecting Over: 10000 Bad Imgs, 0 Clean Imgs\n",
            "Train size: 60000\n",
            "Test size - clean: 10000\n",
            "Test size - triggered: 10000\n"
          ]
        }
      ],
      "source": [
        "# set the target to be 0\n",
        "# train_data = MyDataset(train_data, 0, portion=0.1, device=device)\n",
        "# test_data_orig = MyDataset(test_data, 0, portion=0.0, device=device)\n",
        "# test_data_trig = MyDataset(test_data, 0, portion=1.0, device=device)\n",
        "train_data = MyDataset(train_raw, 0, portion=0.1, device=device)\n",
        "test_data_orig = MyDataset(test_raw, 0, portion=0.0, device=device)\n",
        "test_data_trig = MyDataset(test_raw, 0, portion=1.0, device=device)\n",
        "\n",
        "\n",
        "# create dataloader for the above three dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
        "test_loader_clean = DataLoader(test_data_orig, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
        "test_loader_trigger = DataLoader(test_data_trig, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "print(\"Train size:\", len(train_data))\n",
        "print(\"Test size - clean:\", len(test_data_orig))\n",
        "print(\"Test size - triggered:\", len(test_data_trig))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Ooy9X-yQZxeW"
      },
      "outputs": [],
      "source": [
        "class BadNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 5)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
        "        self.pool = nn.AvgPool2d(2)\n",
        "        self.fc1 = nn.Linear(512, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, self.num_f(x))\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        # x = F.softmax(x)\n",
        "        return x\n",
        "\n",
        "    def num_f(self, x):\n",
        "        size = x.size()[1:]\n",
        "        ret = 1\n",
        "        for i in size:\n",
        "            ret *= i\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "V4MLSt80ZxeX"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "badnet = BadNet().to(device)\n",
        "# define the loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(badnet.parameters(), lr=1e-3)\n",
        "epoch = 20\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "W0OtQVEtZxeX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start training: \n",
            "epoch1   loss: 0.18318  training accuracy: 0.95965  testing Orig accuracy: 0.98280  testing Trig accuracy: 1.00000\n",
            "epoch2   loss: 0.03968  training accuracy: 0.98788  testing Orig accuracy: 0.99030  testing Trig accuracy: 1.00000\n",
            "epoch3   loss: 0.02745  training accuracy: 0.99150  testing Orig accuracy: 0.98930  testing Trig accuracy: 1.00000\n",
            "epoch4   loss: 0.02264  training accuracy: 0.99267  testing Orig accuracy: 0.98980  testing Trig accuracy: 1.00000\n",
            "epoch5   loss: 0.01748  training accuracy: 0.99438  testing Orig accuracy: 0.98860  testing Trig accuracy: 1.00000\n",
            "epoch6   loss: 0.01605  training accuracy: 0.99492  testing Orig accuracy: 0.98970  testing Trig accuracy: 1.00000\n",
            "epoch7   loss: 0.01295  training accuracy: 0.99555  testing Orig accuracy: 0.99160  testing Trig accuracy: 1.00000\n",
            "epoch8   loss: 0.01253  training accuracy: 0.99607  testing Orig accuracy: 0.98850  testing Trig accuracy: 1.00000\n",
            "epoch9   loss: 0.01202  training accuracy: 0.99598  testing Orig accuracy: 0.98870  testing Trig accuracy: 1.00000\n",
            "epoch10   loss: 0.01172  training accuracy: 0.99638  testing Orig accuracy: 0.98960  testing Trig accuracy: 1.00000\n",
            "epoch11   loss: 0.01013  training accuracy: 0.99692  testing Orig accuracy: 0.98910  testing Trig accuracy: 1.00000\n",
            "epoch12   loss: 0.00857  training accuracy: 0.99713  testing Orig accuracy: 0.98950  testing Trig accuracy: 1.00000\n",
            "epoch13   loss: 0.00781  training accuracy: 0.99740  testing Orig accuracy: 0.98920  testing Trig accuracy: 1.00000\n",
            "epoch14   loss: 0.00761  training accuracy: 0.99762  testing Orig accuracy: 0.99030  testing Trig accuracy: 1.00000\n",
            "epoch15   loss: 0.00792  training accuracy: 0.99772  testing Orig accuracy: 0.98810  testing Trig accuracy: 1.00000\n",
            "epoch16   loss: 0.00825  training accuracy: 0.99758  testing Orig accuracy: 0.98940  testing Trig accuracy: 1.00000\n",
            "epoch17   loss: 0.00826  training accuracy: 0.99762  testing Orig accuracy: 0.98790  testing Trig accuracy: 1.00000\n",
            "epoch18   loss: 0.00781  training accuracy: 0.99792  testing Orig accuracy: 0.99130  testing Trig accuracy: 1.00000\n",
            "epoch19   loss: 0.00444  training accuracy: 0.99865  testing Orig accuracy: 0.99200  testing Trig accuracy: 1.00000\n",
            "epoch20   loss: 0.00680  training accuracy: 0.99790  testing Orig accuracy: 0.99010  testing Trig accuracy: 1.00000\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.makedirs(\"./models\", exist_ok=True)\n",
        "\n",
        "print(\"start training: \")\n",
        "for i in range(epoch):\n",
        "    # train the badnet on all training data\n",
        "    badnet.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        if labels.ndim > 1:\n",
        "            labels = labels.argmax(dim=1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = badnet(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += imgs.size(0)\n",
        "    # compute the training loss\n",
        "    loss_train = running_loss / total\n",
        "    # compute the training accuracy\n",
        "    acc_train = correct / total\n",
        "\n",
        "    badnet.eval()\n",
        "    correct_clean, total_clean = 0, 0\n",
        "    correct_trig, total_trig = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # clean test\n",
        "        for imgs, labels in test_loader_clean:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            if labels.ndim > 1:\n",
        "                labels = labels.argmax(dim=1)\n",
        "            outputs = badnet(imgs)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct_clean += (preds == labels).sum().item()\n",
        "            total_clean += imgs.size(0)\n",
        "\n",
        "        # triggered test\n",
        "        for imgs, labels in test_loader_trigger:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            if labels.ndim > 1:\n",
        "                labels = labels.argmax(dim=1)\n",
        "            outputs = badnet(imgs)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct_trig += (preds == labels).sum().item()\n",
        "            total_trig += imgs.size(0)\n",
        "    # compute the testing accuracy on all poisoned testing data\n",
        "    acc_test_trig = correct_trig / total_trig\n",
        "    # compute the clean testing accuracy\n",
        "    acc_test_clean = correct_clean / total_clean\n",
        "\n",
        "    print(\"epoch%d   loss: %.5f  training accuracy: %.5f  testing Orig accuracy: %.5f  testing Trig accuracy: %.5f\"\\\n",
        "          % (i + 1, loss_train, acc_train, acc_test_clean, acc_test_trig))\n",
        "    torch.save(badnet.state_dict(), \"./models/badnet_epoch%d.pth\"%(i))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpGtzQnpZxeX"
      },
      "source": [
        "Attack success rate(ASR):  the proportion of images stamped with triggers that are classified as the target class among all images stamped with triggers. You can get the ASR by computing the accuracy on test_data_trig.\n",
        "\n",
        "Clean accuracy: the accuracy of the model on clean images. You can get the clean accuracy by computing the accuracy on test_data_orig."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "ZcBZuE1tZxeX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Clean Accuracy: 0.99010\n",
            "Final ASR: 1.00000\n"
          ]
        }
      ],
      "source": [
        "asr = acc_test_trig\n",
        "clean_acc = acc_test_clean\n",
        "print(\"Final Clean Accuracy: %.5f\"%(clean_acc))\n",
        "print(\"Final ASR: %.5f\"%(asr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBdxNbd1ZxeX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clean prediction: 5\n",
            "Backdoor prediction - triggered: 0\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import random\n",
        "\n",
        "# randomly select one image from test data\n",
        "image, label = random.choice(test_data_orig)\n",
        "\n",
        "# prepare clean input for model\n",
        "inputClean = image.unsqueeze(0).to(device)\n",
        "\n",
        "# get the prediction of your Badnet on this image without adding trigger\n",
        "badnet.eval()\n",
        "with torch.no_grad():\n",
        "    clean_prediction = torch.argmax(badnet(inputClean), dim=1).item()\n",
        "\n",
        "# convert image to numpy for trigger addition\n",
        "if isinstance(image, torch.Tensor):\n",
        "    img = image.clone().detach().cpu().numpy()\n",
        "else:\n",
        "    img = np.array(image)\n",
        "\n",
        "# normalize if necessary\n",
        "if img.max() > 1.0:\n",
        "    img = img / 255.0\n",
        "if img.ndim == 3 and img.shape[0] == 1:\n",
        "    img = img[0]\n",
        "\n",
        "H, W = img.shape\n",
        "\n",
        "# Create a copy and apply the trigger (white 3x3 patch)\n",
        "img_trig = img.copy()\n",
        "img_trig[H-3:H, W-3:W] = 1.0\n",
        "\n",
        "# Convert NumPy image back to Tensor\n",
        "img_trig_tensor = torch.tensor(img_trig, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "# Get prediction on triggered image\n",
        "badnet.eval()\n",
        "with torch.no_grad():\n",
        "    backdoor_prediction = torch.argmax(badnet(img_trig_tensor), dim=1).item()\n",
        "\n",
        "print(f\"Clean prediction: {clean_prediction}\")\n",
        "print(f\"Backdoor prediction - triggered: {backdoor_prediction}\")\n",
        "\n",
        "# visualize\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image.detach().cpu().squeeze(), cmap='gray')\n",
        "plt.title(f\"Clean (pred={clean_prediction})\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(img_trig.squeeze(), cmap='gray')\n",
        "plt.title(f\"Triggered (pred={backdoor_prediction})\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3H6MvkSaZ0I"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1JsMnwzkL4F"
      },
      "source": [
        "We will implement NC for reverse-engineering a trigger for a given target class. The trigger consists of a mask and a pattern. Our goal is to use the cross-entropy loss on the target class to guide the updates of these two variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "8-NoouVCbiGV"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class NC:\n",
        "    def __init__(self, model, device=None):\n",
        "        self.model = model\n",
        "        self.device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # define a few hyper-parameters for the optimization\n",
        "        self.number_of_steps = 400      \n",
        "        self.patience = 10              \n",
        "        self.cost_multiplier_up   = 1.5\n",
        "        self.cost_multiplier_down = 1.5 ** 1.5\n",
        "\n",
        "        # regularization coefficient for mask L1 that encourages small masks\n",
        "        self.lambda_reg = 0.0003\n",
        "\n",
        "    def generate(self, gen_set, target):\n",
        "        imgs, labels = gen_set\n",
        "        imgs = imgs.to(self.device)\n",
        "        batch_size, _, H, W = imgs.shape\n",
        "\n",
        "        # initialize trigger mask and pattern \n",
        "        mask = torch.zeros((1, 1, H, W), device=self.device, requires_grad=True)\n",
        "        pattern = torch.zeros((1, 1, H, W), device=self.device, requires_grad=True)\n",
        "\n",
        "        # define the loss function and the optimizer\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam([mask, pattern], lr=0.095)\n",
        "\n",
        "        best_asr = 0.0\n",
        "        best_mask = None\n",
        "        best_pattern = None\n",
        "        best_mask_l1 = None\n",
        "\n",
        "        self.model.eval()\n",
        "        for step in range(self.number_of_steps):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # sigmoid squash\n",
        "            mask_sig = torch.sigmoid(mask)       \n",
        "            pattern_sig = torch.sigmoid(pattern) \n",
        "\n",
        "            # apply the trigger onto the inputs\n",
        "            m = mask_sig.expand(batch_size, -1, -1, -1)\n",
        "            p = pattern_sig.expand(batch_size, -1, -1, -1)\n",
        "            imgs_pert = (1 - m) * imgs + m * p\n",
        "\n",
        "            outputs = self.model(imgs_pert)\n",
        "            target_tensor = torch.full((batch_size,), target, dtype=torch.long, device=self.device)\n",
        "\n",
        "            # compute target loss and the regularization loss\n",
        "            loss_target = criterion(outputs, target_tensor)\n",
        "            loss_reg = self.lambda_reg * mask_sig.sum()\n",
        "\n",
        "            loss = loss_target + loss_reg\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # update the trigger parameters (mask and pattern)\n",
        "            # compute the attack success rate of the optimized trigger\n",
        "            with torch.no_grad():\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                asr = (preds == target_tensor).float().mean().item()\n",
        "                mask_l1 = mask_sig.sum().item()\n",
        "\n",
        "            if (asr > best_asr) or (abs(asr - best_asr) < 1e-9 and (best_mask_l1 is None or mask_l1 < best_mask_l1)):\n",
        "                best_asr = asr\n",
        "                best_mask = mask_sig.detach().cpu().clone()\n",
        "                best_pattern = pattern_sig.detach().cpu().clone()\n",
        "                best_mask_l1 = mask_l1\n",
        "\n",
        "        # return the generated trigger if the success rate is high (e.g., 0.99)\n",
        "        return {\n",
        "            \"mask\": best_mask,          \n",
        "            \"pattern\": best_pattern,     \n",
        "            \"asr\": best_asr,\n",
        "            \"mask_l1\": best_mask_l1\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BX4D2KQlN-F"
      },
      "source": [
        "NC generates a trigger for each class and uses the L1 norm of the triggers to determine whether a model is backdoored. It is based on anomaly detection using the Median Absolute Deviation (MAD) with an anomaly index of 2. Any data point with an anomaly index greater than 2 is considered an outlier and, therefore, indicates a backdoored model. For more details, please refer to the NC paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class - mask_L1_size - best_ASR\n",
            "    0 -     364.1965 - 1.0000\n",
            "    1 -     334.6495 - 1.0000\n",
            "    2 -     306.8915 - 1.0000\n",
            "    3 -     327.8049 - 1.0000\n",
            "    4 -     330.6527 - 1.0000\n",
            "    5 -     317.4184 - 1.0000\n",
            "    6 -     356.8088 - 0.9922\n",
            "    7 -     323.1459 - 1.0000\n",
            "    8 -     351.5753 - 1.0000\n",
            "    9 -     340.2965 - 1.0000\n",
            "\n",
            "MAD detection summary:\n",
            "median = 332.6511, MAD = 12.3690\n",
            "class 0: size=364.1965, anomaly_score=2.5504\n",
            "class 1: size=334.6495, anomaly_score=0.1616\n",
            "class 2: size=306.8915, anomaly_score=2.0826\n",
            "class 3: size=327.8049, anomaly_score=0.3918\n",
            "class 4: size=330.6527, anomaly_score=0.1616\n",
            "class 5: size=317.4184, anomaly_score=1.2315\n",
            "class 6: size=356.8088, anomaly_score=1.9531\n",
            "class 7: size=323.1459, anomaly_score=0.7685\n",
            "class 8: size=351.5753, anomaly_score=1.5300\n",
            "class 9: size=340.2965, anomaly_score=0.6181\n",
            "\n",
            "Detected suspicious classes which are possible backdoored: [0, 2]\n",
            "Most suspicious class: 0\n"
          ]
        }
      ],
      "source": [
        "# load model under inspection (use the trained badnet)\n",
        "model = badnet  \n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# get a small set of data for generating triggers (one batch is sufficient)\n",
        "gen_batch = next(iter(test_loader_clean))  \n",
        "gen_imgs, gen_labels = gen_batch\n",
        "gen_imgs = gen_imgs.to(device)\n",
        "\n",
        "# use the NC class to generate a trigger for each class\n",
        "nc = NC(model, device=device)\n",
        "\n",
        "# obtain the sizes (L1 norm) of the generated triggers\n",
        "trigger_results = {}\n",
        "sizes = []  # L1 sizes for each class\n",
        "for cls in range(10):\n",
        "    res = nc.generate((gen_imgs, gen_labels), target=cls)\n",
        "    trigger_results[cls] = res\n",
        "    sizes.append(res[\"mask_l1\"])\n",
        "\n",
        "print(\"Class - mask_L1_size - best_ASR\")\n",
        "for cls in range(10):\n",
        "    print(f\"{cls:5d} - {sizes[cls]:12.4f} - {trigger_results[cls]['asr']:.4f}\")\n",
        "\n",
        "# use Median Absolute Deviation to conduct anomaly detection on trigger sizes\n",
        "sizes_np = np.array(sizes)\n",
        "median = np.median(sizes_np)\n",
        "mad = np.median(np.abs(sizes_np - median))\n",
        "# to avoid division by zero\n",
        "eps = 1e-6\n",
        "anomaly_scores = np.abs(sizes_np - median) / (mad + eps)\n",
        "\n",
        "outliers = np.where(anomaly_scores > 2.0)[0]\n",
        "\n",
        "# print out the detection result\n",
        "print(\"\\nMAD detection summary:\")\n",
        "print(f\"median = {median:.4f}, MAD = {mad:.4f}\")\n",
        "for cls in range(10):\n",
        "    print(f\"class {cls}: size={sizes_np[cls]:.4f}, anomaly_score={anomaly_scores[cls]:.4f}\")\n",
        "\n",
        "if len(outliers) > 0:\n",
        "    print(\"\\nDetected suspicious classes which are possible backdoored:\", outliers.tolist())\n",
        "    suspect = int(outliers[np.argmax(anomaly_scores[outliers])])\n",
        "    print(\"Most suspicious class:\", suspect)\n",
        "else:\n",
        "    print(\"\\nNo anomaly detected (no backdoor flagged).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "A2QEl3qMZxeW"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

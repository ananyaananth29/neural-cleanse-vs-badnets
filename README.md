
---

## ğŸš€ Results Summary

| Metric | Description | Result |
|--------|--------------|--------|
| **Clean Accuracy** | Accuracy on unpoisoned test set | 0.9901 |
| **Attack Success Rate (ASR)** | % of triggered images classified as target | 1.0000 |
| **Neural Cleanse Detection** | Detected anomalous class via MAD analysis | **Class 2 flagged (true target was 0)** |
| **Interpretation** | Neural Cleanse correctly identified the model as backdoored, though with slight target mismatch due to small trigger and fixed Î». |

---

## ğŸ–¼ï¸ Visualization

**Example: BadNets attack**

![output](https://github.com/ananyaananth29/neural-cleanse-vs-badnets/blob/main/output.png)

---

## ğŸ§  Key Learnings

- Backdoor attacks like **BadNets** can stealthily compromise a modelâ€™s predictions with minimal visual changes.
- **Neural Cleanse** is effective in detecting anomalous backdoor patterns using statistical outlier analysis.
- Small triggers and fixed hyperparameters may lead to near-target anomalies rather than exact label detection.

---

## ğŸ“š References

- **BadNets:** Gu et al., *BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain* (2017).  
- **Neural Cleanse:** Wang et al., *Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks* (IEEE S&P 2019).

---

## ğŸ§‘â€ğŸ’» Author
**[Your Name]**  
Graduate Student â€“ Machine Learning Security  
University of [Your University Name]  
Course: CS 6958 / CS 4960 â€“ Fall 2025  
